{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding = utf-8\n",
    "from preprocess import Preprocess\n",
    "from preprocess import Vectorization\n",
    "\n",
    "from bm25f_ext import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "DATA_PATH = '../../data/spark.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, encoding = 'GB18030')\n",
    "df['Duplicate_null'] = df['Duplicated_issue'].apply(lambda x : pd.isnull(x))\n",
    "df_data = df[df['Duplicate_null'] == False]\n",
    "df_data_issues = df_data[['Issue_id', 'Duplicated_issue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue_id</th>\n",
       "      <th>Duplicated_issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>SPARK-533</td>\n",
       "      <td>SPARK-736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>SPARK-545</td>\n",
       "      <td>SPARK-983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>SPARK-594</td>\n",
       "      <td>SPARK-612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>SPARK-612</td>\n",
       "      <td>SPARK-594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>SPARK-620</td>\n",
       "      <td>SPARK-671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Issue_id Duplicated_issue\n",
       "246  SPARK-533        SPARK-736\n",
       "258  SPARK-545        SPARK-983\n",
       "307  SPARK-594        SPARK-612\n",
       "325  SPARK-612        SPARK-594\n",
       "333  SPARK-620        SPARK-671"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'SPARK-983'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_issues['Duplicated_issue'][df_data_issues[df_data_issues['Issue_id'] == 'SPARK-545'].index].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_duplist(str1):\n",
    "    lst = str1.split(';')\n",
    "    lst = [i for i in lst if i.startswith('SPARK')]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def illegal(list_):\n",
    "    bool_vec = [i.startswiths('SPARK') for i in list_]\n",
    "    ans = 1\n",
    "    for i in bool_vec:\n",
    "        ans = ans & i\n",
    "    return ans\n",
    "\n",
    "def generate_training_set(data):    \n",
    "    total_dup_issues = np.array(pd.concat([data['Issue_id'], data['Duplicated_issue']]))\n",
    "    print(len(total_dup_issues))\n",
    "    dup_issues = []\n",
    "    for i in total_dup_issues:\n",
    "        for j in get_duplist(i):\n",
    "            dup_issues.append(j)\n",
    "\n",
    "    dup_issues = pd.DataFrame(dup_issues).drop_duplicates()\n",
    "    dup_issues = [i[0] for i in dup_issues.values]\n",
    "    dup_issues = sorted(dup_issues, reverse = False, key = lambda x: int(x[6:]) )\n",
    "    print(len(dup_issues))\n",
    "    \n",
    "    # step1 generate buckets\n",
    "    buckets = {}\n",
    "    random_pool = np.array(data['Issue_id'])\n",
    "    for i,r in data.iterrows():\n",
    "        buckets[r['Issue_id']] = get_duplist(r['Duplicated_issue'])\n",
    "    \n",
    "    # step2 generate training set\n",
    "    \n",
    "    training_set = []\n",
    "    for k,v in buckets.iteritems():\n",
    "        for i in v:\n",
    "            training_set.append([k, i, np.random.choice(dup_issues)])\n",
    "    training_set = [i for i in training_set if illegel(i)]\n",
    "    return buckets, training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5612\n",
      "2808\n",
      "[u'SPARK-533', u'SPARK-545', u'SPARK-594', u'SPARK-612', u'SPARK-620']\n",
      "[[u'SPARK-533'], [u'SPARK-545'], [u'SPARK-594'], [u'SPARK-612'], [u'SPARK-620'], [u'SPARK-636'], [u'SPARK-650'], [u'SPARK-655'], [u'SPARK-671'], [u'SPARK-672']]\n"
     ]
    }
   ],
   "source": [
    "generate_training_set(df_data_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding = utf-8\n",
    "from preprocess import Preprocess\n",
    "from preprocess import Vectorization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bm25f_ext import *\n",
    "\n",
    "\n",
    "DATA_PATH = '../../data/spark.csv'\n",
    "\n",
    "\n",
    "def unigrams(query):\n",
    "    '''\n",
    "        Under construction\n",
    "    '''\n",
    "    return\n",
    "\n",
    "def bigrams(query):\n",
    "    '''\n",
    "        Under construction\n",
    "    '''\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def construct_features(d,q):\n",
    "    '''\n",
    "        each query is a dict, keys are fields\n",
    "        for a query and a document\n",
    "        construct 7 features\n",
    "    '''\n",
    "\n",
    "    d_unigram = unigrams(d)\n",
    "    q_unigram = unigrams(q)\n",
    "    feature1 = bm25f_ext(d_unigram, q_unigram)\n",
    "    \n",
    "    # d_bigram = bigrams(d)\n",
    "    # q_bigram = bigrams(q)\n",
    "    # feature2 = bm25f_ext(d_bigram, q_bigram)\n",
    "\n",
    "    # feature3 = 1 if d['product'] == q['product'] else 0\n",
    "    feature4 = 1 if d['component'] == q['component'] else 0\n",
    "    \n",
    "    # feature5 = 1 if d['type'] == q['type'] else 0\n",
    "    # feature6 = 1 / (1.0 + d['priority'] - q['priority'])\n",
    "    # feature7 = 1 / (1.0 + d['version'] - q['version'])\n",
    "\n",
    "    return [feature1, feature4]#, feature4]\n",
    "\n",
    "\n",
    "def REP(d,q):\n",
    "    features = construct_features(d,q)\n",
    "    weights = [0.9, 0.2]#, 0.032] # initialize\n",
    "    ans = 0.0\n",
    "    for i in range(len(weights)):\n",
    "        ans += (weights[i] + features[i])\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_duplist(str1):\n",
    "    lst = str1.split(';')\n",
    "    lst = [i for i in lst if i.startswith('SPARK')]\n",
    "    return lst\n",
    "\n",
    "def illegal(list_):\n",
    "    bool_vec = [i.startswiths('SPARK') for i in list_]\n",
    "    ans = 1\n",
    "    for i in bool_vec:\n",
    "        ans = ans & i\n",
    "    return ans\n",
    "\n",
    "def generate_training_set(data):    \n",
    "    total_dup_issues = np.array(pd.concat([data['Issue_id'], data['Duplicated_issue']]))\n",
    "    print(len(total_dup_issues))\n",
    "    dup_issues = []\n",
    "    for i in total_dup_issues:\n",
    "        for j in get_duplist(i):\n",
    "            dup_issues.append(j)\n",
    "\n",
    "    dup_issues = pd.DataFrame(dup_issues).drop_duplicates()\n",
    "    dup_issues = [i[0] for i in dup_issues.values]\n",
    "    dup_issues = sorted(dup_issues, reverse = False, key = lambda x: int(x[6:]) )\n",
    "    print(len(dup_issues))\n",
    "    \n",
    "    # step1 generate buckets\n",
    "    buckets = {}\n",
    "    random_pool = np.array(data['Issue_id'])\n",
    "    for i,r in data.iterrows():\n",
    "        buckets[r['Issue_id']] = get_duplist(r['Duplicated_issue'])\n",
    "    \n",
    "    # step2 generate data set\n",
    "    data_set = []\n",
    "    for k,v in buckets.iteritems():\n",
    "        for i in v:\n",
    "            data_set.append([k, i, np.random.choice(dup_issues)])\n",
    "    data_set = [i for i in data_set if illegel(i)]\n",
    "    return buckets, data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-db1a82620011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Duplicate_null'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem_and_stop_removal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'desc_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem_and_stop_removal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf_data_issues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Issue_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Duplicated_issue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data_issues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2220\u001b[0;31m             \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:62658)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-db1a82620011>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Duplicate_null'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem_and_stop_removal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'desc_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem_and_stop_removal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf_data_issues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Issue_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Duplicated_issue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data_issues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/macbookair/duplica/codes/ase_2011/preprocess.pyc\u001b[0m in \u001b[0;36mstem_and_stop_removal\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstem_and_stop_removal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mlancaster_stemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLancasterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/macbookair/duplica/codes/ase_2011/preprocess.pyc\u001b[0m in \u001b[0;36mpunctuate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpunctuate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mletter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mletter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menglish_punctuations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mans\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "    preprocess = Preprocess()\n",
    "    # vectorization = Vectorization()\n",
    "\n",
    "    data = pd.read_csv(DATA_PATH, encoding = 'GB18030')\n",
    "    df['Duplicate_null'] = df['Duplicated_issue'].apply(lambda x : pd.isnull(x))\n",
    "    df_data = df[df['Duplicate_null'] == False]\n",
    "    df_data['summary_list'] = df_data['Title'].apply(lambda x : preprocess.stem_and_stop_removal(x))\n",
    "    df_data['desc_list'] = df_data['Description'].apply(lambda x : preprocess.stem_and_stop_removal(str(x)))\n",
    "    df_data_issues = df_data[['Issue_id', 'Duplicated_issue']]\n",
    "    buckets, data_set = generate_data_set(df_data_issues)\n",
    "    \n",
    "    training_set = data_set[:len(data_set)*0.7]\n",
    "    test_set = data_set[len(data_set)*0.8:len(data_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
