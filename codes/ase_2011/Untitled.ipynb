{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding = utf-8\n",
    "from preprocess import Preprocess\n",
    "from preprocess import Vectorization\n",
    "\n",
    "from bm25f_ext import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "DATA_PATH = '../../data/spark.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, encoding = 'GB18030')\n",
    "df['Duplicate_null'] = df['Duplicated_issue'].apply(lambda x : pd.isnull(x))\n",
    "df_data = df[df['Duplicate_null'] == False]\n",
    "df_data_issues = df_data[['Issue_id', 'Duplicated_issue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue_id</th>\n",
       "      <th>Duplicated_issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>SPARK-533</td>\n",
       "      <td>SPARK-736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>SPARK-545</td>\n",
       "      <td>SPARK-983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>SPARK-594</td>\n",
       "      <td>SPARK-612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>SPARK-612</td>\n",
       "      <td>SPARK-594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>SPARK-620</td>\n",
       "      <td>SPARK-671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Issue_id Duplicated_issue\n",
       "246  SPARK-533        SPARK-736\n",
       "258  SPARK-545        SPARK-983\n",
       "307  SPARK-594        SPARK-612\n",
       "325  SPARK-612        SPARK-594\n",
       "333  SPARK-620        SPARK-671"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'SPARK-983'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_issues['Duplicated_issue'][df_data_issues[df_data_issues['Issue_id'] == 'SPARK-545'].index].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_duplist(str1):\n",
    "    lst = str1.split(';')\n",
    "    lst = [i for i in lst if i.startswith('SPARK')]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def illegal(list_):\n",
    "    bool_vec = [i.startswiths('SPARK') for i in list_]\n",
    "    ans = 1\n",
    "    for i in bool_vec:\n",
    "        ans = ans & i\n",
    "    return ans\n",
    "\n",
    "def generate_training_set(data):    \n",
    "    total_dup_issues = np.array(pd.concat([data['Issue_id'], data['Duplicated_issue']]))\n",
    "    print(len(total_dup_issues))\n",
    "    dup_issues = []\n",
    "    for i in total_dup_issues:\n",
    "        for j in get_duplist(i):\n",
    "            dup_issues.append(j)\n",
    "\n",
    "    dup_issues = pd.DataFrame(dup_issues).drop_duplicates()\n",
    "    dup_issues = [i[0] for i in dup_issues.values]\n",
    "    dup_issues = sorted(dup_issues, reverse = False, key = lambda x: int(x[6:]) )\n",
    "    print(len(dup_issues))\n",
    "    \n",
    "    # step1 generate buckets\n",
    "    buckets = {}\n",
    "    random_pool = np.array(data['Issue_id'])\n",
    "    for i,r in data.iterrows():\n",
    "        buckets[r['Issue_id']] = get_duplist(r['Duplicated_issue'])\n",
    "    \n",
    "    # step2 generate training set\n",
    "    \n",
    "    training_set = []\n",
    "    for k,v in buckets.iteritems():\n",
    "        for i in v:\n",
    "            training_set.append([k, i, np.random.choice(dup_issues)])\n",
    "    training_set = [i for i in training_set if illegel(i)]\n",
    "    return buckets, training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5612\n",
      "2808\n",
      "[u'SPARK-533', u'SPARK-545', u'SPARK-594', u'SPARK-612', u'SPARK-620']\n",
      "[[u'SPARK-533'], [u'SPARK-545'], [u'SPARK-594'], [u'SPARK-612'], [u'SPARK-620'], [u'SPARK-636'], [u'SPARK-650'], [u'SPARK-655'], [u'SPARK-671'], [u'SPARK-672']]\n"
     ]
    }
   ],
   "source": [
    "generate_training_set(df_data_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding = utf-8\n",
    "from preprocess import Preprocess\n",
    "from preprocess import Vectorization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bm25f_ext import *\n",
    "\n",
    "\n",
    "DATA_PATH = '../../data/spark.csv'\n",
    "\n",
    "\n",
    "def unigrams(query):\n",
    "    '''\n",
    "        Under construction\n",
    "    '''\n",
    "    return\n",
    "\n",
    "def bigrams(query):\n",
    "    '''\n",
    "        Under construction\n",
    "    '''\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def construct_features(d,q, corpus):\n",
    "    '''\n",
    "        each query is a dict, keys are fields\n",
    "        for a query and a document\n",
    "        construct 7 features\n",
    "    '''\n",
    "\n",
    "    d_unigram = d\n",
    "    q_unigram = q\n",
    "    feature1 = bm25f_ext(d_unigram, q_unigram, corpus)\n",
    "    \n",
    "    # d_bigram = bigrams(d)\n",
    "    # q_bigram = bigrams(q)\n",
    "    # feature2 = bm25f_ext(d_bigram, q_bigram)\n",
    "\n",
    "    # feature3 = 1 if d['product'] == q['product'] else 0\n",
    "    feature4 = 1 if d['component'] == q['component'] else 0\n",
    "    \n",
    "    # feature5 = 1 if d['type'] == q['type'] else 0\n",
    "    # feature6 = 1 / (1.0 + d['priority'] - q['priority'])\n",
    "    # feature7 = 1 / (1.0 + d['version'] - q['version'])\n",
    "\n",
    "    return [feature1, feature4]#, feature4]\n",
    "\n",
    "\n",
    "def REP(d,q, corpus):\n",
    "    features = construct_features(d,q, corpus)\n",
    "    weights = [0.9, 0.2]#, 0.032] # initialize\n",
    "    ans = 0.0\n",
    "    for i in range(len(weights)):\n",
    "        ans += (weights[i] + features[i])\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_duplist(str1):\n",
    "    lst = str1.split(';')\n",
    "    lst = [i for i in lst if i.startswith('SPARK')]\n",
    "    return lst\n",
    "\n",
    "def illegal(list_):\n",
    "    bool_vec = [str(i).startswith('SPARK') for i in list_]\n",
    "    ans = True\n",
    "    for i in bool_vec:\n",
    "        ans = ans & i\n",
    "    return ans\n",
    "    \n",
    "def generate_data_set(data):    \n",
    "    total_dup_issues = np.array(pd.concat([data['Issue_id'], data['Duplicated_issue']]))\n",
    "    print(len(total_dup_issues))\n",
    "    dup_issues = []\n",
    "    for i in total_dup_issues:\n",
    "        for j in get_duplist(i):\n",
    "            dup_issues.append(j)\n",
    "\n",
    "    dup_issues = pd.DataFrame(dup_issues).drop_duplicates()\n",
    "    dup_issues = [i[0] for i in dup_issues.values]\n",
    "    dup_issues = sorted(dup_issues, reverse = False, key = lambda x: int(x[6:]) )\n",
    "    print(len(dup_issues))\n",
    "    \n",
    "    # step1 generate buckets\n",
    "    buckets = {}\n",
    "    random_pool = np.array(data['Issue_id'])\n",
    "    for i,r in data.iterrows():\n",
    "        buckets[r['Issue_id']] = get_duplist(r['Duplicated_issue'])\n",
    "    \n",
    "    # step2 generate data set\n",
    "    data_set = []\n",
    "    for k,v in buckets.iteritems():\n",
    "        for i in v:\n",
    "            data_set.append([k, i, np.random.choice(dup_issues)])\n",
    "    print(data_set[0])\n",
    "    data_set = [i for i in data_set if illegal(i)]\n",
    "    print(len(data_set))\n",
    "    return buckets, data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "    preprocess = Preprocess()\n",
    "    # vectorization = Vectorization()\n",
    "\n",
    "    df = pd.read_csv(DATA_PATH, encoding = 'GB18030')\n",
    "    df['Duplicate_null'] = df['Duplicated_issue'].apply(lambda x : pd.isnull(x))\n",
    "    df_data = df[df['Duplicate_null'] == False]\n",
    "    df_data['summary_list'] = df_data['Title'].apply(lambda x : preprocess.stem_and_stop_removal(x))\n",
    "    df_data['desc_list'] = df_data['Description'].apply(lambda x : preprocess.stem_and_stop_removal(str(x)))\n",
    "    df_data_issues = df_data[['Issue_id', 'Duplicated_issue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5612\n",
      "2808\n",
      "[u'SPARK-18270', u'SPARK-16472', u'SPARK-16901']\n",
      "3177\n"
     ]
    }
   ],
   "source": [
    "buckets, data_set = generate_data_set(df_data_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set = data_set[:int(len(data_set)*0.7)]\n",
    "test_set = data_set[int(0.7*len(data_set)):int(0.8*len(data_set))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # unigram:\n",
    "    training_data = []\n",
    "    for q in training_set[:5]:\n",
    "        tmp = []\n",
    "        for j in q:\n",
    "            tmp_dict = {}\n",
    "            smry = df_data['summary_list'][df_data[df_data['Issue_id'] == j].index].values[0]\n",
    "            tmp_dict['summary'] = smry\n",
    "            desc = df_data['desc_list'][df_data[df_data['Issue_id'] == j].index].values[0]\n",
    "            tmp_dict['description'] = desc\n",
    "            tmp_dict['component'] = df_data['Component'][df_data[df_data['Issue_id'] == j].index].values[0]\n",
    "            tmp.append(tmp_dict)\n",
    "\n",
    "        training_data.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for j in df_data['desc_list'].values:\n",
    "    corpus += j\n",
    "for j in df_data['summary_list'].values:\n",
    "    corpus += j\n",
    "corpus = list(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def RNC(training_data, corpus):\n",
    "    front = REP(training_data[0], training_data[1], corpus)\n",
    "    rare = REP(training_data[1], training_data[2], corpus)\n",
    "    Y = front - rare\n",
    "    return math.log(1 + math.exp(Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5]\n"
     ]
    }
   ],
   "source": [
    "def generate_bigram(a):\n",
    "    bi = []\n",
    "    for i in range(len(a)-1):\n",
    "        bi.append(a[i]+a[i+1])\n",
    "    return bi\n",
    "\n",
    "a = [12,3]\n",
    "print generate_bigram(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[2, 3, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0          1\n",
       "0  1  [2, 3, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "b =[[1, [2,3,4]]]\n",
    "pd.DataFrame(b)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
